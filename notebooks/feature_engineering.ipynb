{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projecting Food Insecurity Rates in the US by County\n",
    "## Feature Engineering\n",
    "The following process imports a cleaned dataset produced from [cleaning_pt2.ipynb.](notebooks/cleaning_pt2) This notebook is used to produce new features that will be used in the modeling process.\n",
    "### Flatiron School Data Science Capstone<br>By Khyatee Desai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "pd.set_option('display.max_columns', None)\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('../pickled/fully_cleaned_data.pickle', \"rb\") as input_file:\n",
    "    df = pickle.load(input_file) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a percentage for each demographic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace(-888888888.0,np.nan,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Percent_male'] = df['TOT_MALE']/df['TOT_POP']\n",
    "df['Percent_female'] = df['TOT_FEMALE']/df['TOT_POP']\n",
    "df['Percent_white'] = df['TOT_WHITE']/df['TOT_POP']\n",
    "df['Percent_Black'] = df['TOT_BLACK']/df['TOT_POP']\n",
    "df['Percent_native'] = df['TOT_NATIVE']/df['TOT_POP']\n",
    "df['Percent_asian'] = df['TOT_ASIAN']/df['TOT_POP']\n",
    "df['Percent_pacific'] = df['TOT_PACIFIC']/df['TOT_POP']\n",
    "df['Percent_latinX'] = df['TOT_LATINX']/df['TOT_POP']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percentage PoC\n",
    "Percentage of a county that is not white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Percent_PoC'] = 1-df['Percent_white']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workforce as a percentage of total population\n",
    "Workforce represented as percentage, so that it can be compared across different counties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percent of population that is working\n",
    "df['Percent_working'] = df['Total_workforce']/df['TOT_POP']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum of Food Establishments\n",
    "Total number of food retail businesses, which is the sum of the three categories (wholesalers, restaraunts, and grocery stores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Total_food_retail'] = df['Num_wholesale'].fillna(0)+ df['Num_restaraunts'].fillna(0)+df['Num_grocery'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Population divided by number of food establishments \n",
    "Looking at prevalence of food establishments as a function of population - aka how many Food Retail establishments exist per person within a County"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Food_retail_per_person'] = df['Total_food_retail']/df['TOT_POP']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACS Survey Percentages\n",
    "Take percentages of all features taken from the ACS survey, which use a slightly different (~5%) total population number for the denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Percent_disabled'] = df['pop_disabled']/df['TOT_POP']\n",
    "df['Percent_hs_grad'] = df['pop_hs_grad']/df['TOT_POP']\n",
    "df['Percent_bachelors'] = df['pop_bachelors']/df['TOT_POP']\n",
    "df['Percent_grad_degree'] = df['pop_grad_degree']/df['TOT_POP']\n",
    "df['Percent_priv_health'] = df['pop_priv_health']/df['TOT_POP']\n",
    "df['Percent_public_health'] = df['pop_public_health']/df['TOT_POP']\n",
    "df['Percent_no_health'] = df['pop_no_health']/df['TOT_POP']\n",
    "df['Percent_65+'] = df['pop_65+']/df['TOT_POP']\n",
    "df['Percent_non_citizen'] = df['pop_non_citizen']/df['TOT_POP']\n",
    "df['Percent_hh_no_vehicle'] = df['hh_no_vehicle']/df['num_hh']\n",
    "df['Percent_hh_SNAP'] = df['hh_SNAP']/df['num_hh']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop raw count columns after deriving percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['TOT_MALE','TOT_FEMALE','TOT_WHITE','TOT_BLACK','TOT_NATIVE','TOT_ASIAN','TOT_PACIFIC','TOT_LATINX',\n",
    "         'pop_disabled', 'pop_hs_grad','pop_bachelors', 'pop_grad_degree', 'pop_priv_health', 'pop_public_health',\n",
    "        'pop_no_health', 'pop_65+','pop_non_citizen','hh_no_vehicle','hh_SNAP','pop_total'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Percentages for CPS 2020 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pickled/cps_20_data.pickle', \"rb\") as input_file:\n",
    "    df_cps_20 = pickle.load(input_file) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Education Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cps_20['Percent_hs_grad'] = df_cps_20['pop_hs_grad'] / df_cps_20['Num_respondants_b']\n",
    "df_cps_20['Percent_bachelors'] = df_cps_20['pop_bachelors'] / df_cps_20['Num_respondants_b']\n",
    "df_cps_20['Percent_grad_degree'] = df_cps_20['pop_grad_degree'] / df_cps_20['Num_respondants_b']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Citizenship Status Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_cps_20['Percent_non_citizen'] = df_cps_20['pop_non_citizen'] / df_cps_20['Num_respondants_b']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disability Status Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cps_20['Percent_disabled'] = df_cps_20['HH_disabled']/ (df_cps_20['HH_disabled']+df_cps_20['HH_not_disabled'] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Health Insurance columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cps_20['Percent_no_health'] = (df_cps_20['HH_not_insured'])/(df_cps_20['HH_health_insured']+df_cps_20['HH_not_insured']+df_cps_20['HH_some_insured'])\n",
    "df_cps_20['Percent_priv_health'] = (df_cps_20['HH_health_priv']+df_cps_20['HH_some_insured_priv']) /(df_cps_20['HH_not_insured_priv'] + df_cps_20['HH_some_insured_priv']+df_cps_20['HH_health_priv'])\n",
    "df_cps_20['Percent_public_health'] = (df_cps_20['HH_insured_pub']+df_cps_20['HH_some_insured_pub']) /(df_cps_20['HH_no_health_pub'] + df_cps_20['HH_some_insured_pub']+df_cps_20['HH_insured_pub'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cps_20 = df_cps_20.loc[:,['Year','HH_income','HH_size', 'Percent_hs_grad','Percent_bachelors','Percent_grad_degree',\n",
    "    'Percent_non_citizen','Percent_disabled','Percent_no_health','Percent_priv_health','Percent_public_health']].reset_index()\n",
    "\n",
    "df_cps_20.rename(columns={'index':'FIPS', 'HH_income':'hh_med_income','HH_size':'hh_avg_size' },inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate 2020 CPS data with main df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_no_20 = df[~df.Year.isin(['2020'])]\n",
    "df_20 = df[df.Year=='2020']\n",
    "\n",
    "df_20_dropped = df_20.drop(df_cps_20.columns[1:],axis=1)\n",
    "df_20 = df_cps_20.merge(df_20_dropped, on='FIPS', how='outer')\n",
    "\n",
    "df = pd.concat([df_no_20, df_20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute Missing 2020 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Percent_male</th>\n",
       "      <th>Percent_female</th>\n",
       "      <th>Percent_white</th>\n",
       "      <th>Percent_Black</th>\n",
       "      <th>Percent_native</th>\n",
       "      <th>Percent_asian</th>\n",
       "      <th>Percent_pacific</th>\n",
       "      <th>Percent_latinX</th>\n",
       "      <th>Percent_PoC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>0.499638</td>\n",
       "      <td>0.500362</td>\n",
       "      <td>0.857873</td>\n",
       "      <td>0.089985</td>\n",
       "      <td>0.021619</td>\n",
       "      <td>0.012652</td>\n",
       "      <td>0.001112</td>\n",
       "      <td>0.084234</td>\n",
       "      <td>0.142127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>0.499541</td>\n",
       "      <td>0.500459</td>\n",
       "      <td>0.839700</td>\n",
       "      <td>0.110203</td>\n",
       "      <td>0.018216</td>\n",
       "      <td>0.013882</td>\n",
       "      <td>0.001130</td>\n",
       "      <td>0.064932</td>\n",
       "      <td>0.160300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>0.499773</td>\n",
       "      <td>0.500227</td>\n",
       "      <td>0.838104</td>\n",
       "      <td>0.110548</td>\n",
       "      <td>0.018413</td>\n",
       "      <td>0.014356</td>\n",
       "      <td>0.001165</td>\n",
       "      <td>0.066211</td>\n",
       "      <td>0.161896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>0.499853</td>\n",
       "      <td>0.500147</td>\n",
       "      <td>0.835338</td>\n",
       "      <td>0.111947</td>\n",
       "      <td>0.018647</td>\n",
       "      <td>0.014926</td>\n",
       "      <td>0.001204</td>\n",
       "      <td>0.067619</td>\n",
       "      <td>0.164662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>0.500625</td>\n",
       "      <td>0.499375</td>\n",
       "      <td>0.850215</td>\n",
       "      <td>0.092106</td>\n",
       "      <td>0.022659</td>\n",
       "      <td>0.014644</td>\n",
       "      <td>0.001266</td>\n",
       "      <td>0.091117</td>\n",
       "      <td>0.149785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>0.500818</td>\n",
       "      <td>0.499182</td>\n",
       "      <td>0.848638</td>\n",
       "      <td>0.092428</td>\n",
       "      <td>0.022878</td>\n",
       "      <td>0.015110</td>\n",
       "      <td>0.001301</td>\n",
       "      <td>0.092774</td>\n",
       "      <td>0.151362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>0.500852</td>\n",
       "      <td>0.499148</td>\n",
       "      <td>0.847006</td>\n",
       "      <td>0.092782</td>\n",
       "      <td>0.023124</td>\n",
       "      <td>0.015556</td>\n",
       "      <td>0.001332</td>\n",
       "      <td>0.094446</td>\n",
       "      <td>0.152994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>0.500875</td>\n",
       "      <td>0.499125</td>\n",
       "      <td>0.845457</td>\n",
       "      <td>0.093154</td>\n",
       "      <td>0.023354</td>\n",
       "      <td>0.015933</td>\n",
       "      <td>0.001368</td>\n",
       "      <td>0.096121</td>\n",
       "      <td>0.154543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>0.500904</td>\n",
       "      <td>0.499096</td>\n",
       "      <td>0.843955</td>\n",
       "      <td>0.093561</td>\n",
       "      <td>0.023547</td>\n",
       "      <td>0.016242</td>\n",
       "      <td>0.001411</td>\n",
       "      <td>0.097676</td>\n",
       "      <td>0.156045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>0.501006</td>\n",
       "      <td>0.498994</td>\n",
       "      <td>0.842628</td>\n",
       "      <td>0.093893</td>\n",
       "      <td>0.023743</td>\n",
       "      <td>0.016548</td>\n",
       "      <td>0.001342</td>\n",
       "      <td>0.099105</td>\n",
       "      <td>0.157372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Percent_male  Percent_female  Percent_white  Percent_Black  \\\n",
       "Year                                                               \n",
       "2009           NaN             NaN            NaN            NaN   \n",
       "2010      0.499638        0.500362       0.857873       0.089985   \n",
       "2011      0.499541        0.500459       0.839700       0.110203   \n",
       "2012      0.499773        0.500227       0.838104       0.110548   \n",
       "2013      0.499853        0.500147       0.835338       0.111947   \n",
       "2014      0.500625        0.499375       0.850215       0.092106   \n",
       "2015      0.500818        0.499182       0.848638       0.092428   \n",
       "2016      0.500852        0.499148       0.847006       0.092782   \n",
       "2017      0.500875        0.499125       0.845457       0.093154   \n",
       "2018      0.500904        0.499096       0.843955       0.093561   \n",
       "2019      0.501006        0.498994       0.842628       0.093893   \n",
       "2020           NaN             NaN            NaN            NaN   \n",
       "\n",
       "      Percent_native  Percent_asian  Percent_pacific  Percent_latinX  \\\n",
       "Year                                                                   \n",
       "2009             NaN            NaN              NaN             NaN   \n",
       "2010        0.021619       0.012652         0.001112        0.084234   \n",
       "2011        0.018216       0.013882         0.001130        0.064932   \n",
       "2012        0.018413       0.014356         0.001165        0.066211   \n",
       "2013        0.018647       0.014926         0.001204        0.067619   \n",
       "2014        0.022659       0.014644         0.001266        0.091117   \n",
       "2015        0.022878       0.015110         0.001301        0.092774   \n",
       "2016        0.023124       0.015556         0.001332        0.094446   \n",
       "2017        0.023354       0.015933         0.001368        0.096121   \n",
       "2018        0.023547       0.016242         0.001411        0.097676   \n",
       "2019        0.023743       0.016548         0.001342        0.099105   \n",
       "2020             NaN            NaN              NaN             NaN   \n",
       "\n",
       "      Percent_PoC  \n",
       "Year               \n",
       "2009          NaN  \n",
       "2010     0.142127  \n",
       "2011     0.160300  \n",
       "2012     0.161896  \n",
       "2013     0.164662  \n",
       "2014     0.149785  \n",
       "2015     0.151362  \n",
       "2016     0.152994  \n",
       "2017     0.154543  \n",
       "2018     0.156045  \n",
       "2019     0.157372  \n",
       "2020          NaN  "
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('Year')[['Percent_male','Percent_female','Percent_white','Percent_Black','Percent_native',\n",
    "                    'Percent_asian','Percent_pacific','Percent_latinX','Percent_PoC']].mean()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Features\n",
    "**Note:** Decided to omit polynomial features, because they decreased model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only using derived percentages, ignore raw counts\n",
    "continuous_features = ['Rent', 'Houseless_rate','Sheltered_rate', 'Unsheltered_rate', 'TOT_POP',\n",
    "       'Cost Per Meal', 'Num_wholesale','Num_restaraunts', 'Num_grocery',  'Unemployment_rate', 'Percent_male', \n",
    "         'Percent_female','Percent_white', 'Percent_Black', 'Percent_native', 'Percent_asian',\n",
    "       'Percent_pacific', 'Percent_latinX','Percent_working', 'Total_food_retail','Food_retail_per_person',\n",
    "        'Percent_disabled','Percent_hs_grad','Percent_bachelors','Percent_grad_degree','Percent_priv_health',\n",
    "            'Percent_public_health','Percent_no_health','Percent_65+','Percent_non_citizen','Percent_hh_no_vehicle',\n",
    "                    'Percent_hh_SNAP','percent_hh_poverty','hh_avg_size','num_hh','hh_med_income']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## add squared and cubed polynomials for each continuous feature\n",
    "# for feat in continuous_features:\n",
    "#     df[feat+'^2'] = df[feat]**2\n",
    "#     df[feat+'^3'] = df[feat]**3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interaction Features\n",
    "Create an interaction feature for each combination of continuous features, and add best ones to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate combinations of features\n",
    "y = df.dropna()[['FI Rate']]\n",
    "X = df.dropna()[continuous_features]\n",
    "interactions = list(combinations(X.columns, 2))\n",
    "interaction_dict = {}\n",
    "\n",
    "# run simple regression model with each possible interaction, and save R-squared for each interaction in a dictionary\n",
    "for interaction in interactions:\n",
    "    X_copy = X.copy()\n",
    "    X_copy['interact'] = X_copy[interaction[0]] * X_copy[interaction[1]] \n",
    "    X_copy = X_copy.replace([np.inf, -np.inf], 0)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_copy, y)\n",
    "    interaction_dict[model.score(X_copy, y)] = interaction \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add best 50 interactions to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the interactions dictionary, and add best 50 interactions to dataframe\n",
    "top_interactions = sorted(interaction_dict.keys(), reverse = True)[:50]\n",
    "for interaction in top_interactions:\n",
    "    feature1 = interaction_dict[interaction][0]\n",
    "    feature2 = interaction_dict[interaction][1]\n",
    "    df[feature1+'_X_'+feature2] = df[feature1] * df[feature2] #also add to new_features df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log Transformations\n",
    "Take natural log of each continuous feature, and add these log features to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feat in continuous_features:\n",
    "    df['log_'+feat] = df[feat].map(lambda x: np.log(x))\n",
    "df = df.replace([np.inf, -np.inf], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy Variables\n",
    "High and Low Threshold programs delineate the assistance programs provided by State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy variables for high and low threshold programs, and add to dataframe\n",
    "hi_thresh_dummies = pd.get_dummies(df['High Threshold Type'].astype(str), dtype=int)\n",
    "hi_thresh_dummies['other'] = hi_thresh_dummies['Other Nutrition Program'] + hi_thresh_dummies['other nutrition pgm']\n",
    "hi_thresh_dummies.drop(['Other Nutrition Program','other nutrition pgm','nan'],axis=1,inplace=True) # drop last col\n",
    "hi_thresh_dummies.rename(columns = {'SNAP, Other Nutrition Programs': 'SNAP_other'}, inplace=True)\n",
    "hi_thresh_dummies.columns = 'Hi_thresh_'+hi_thresh_dummies.columns\n",
    "\n",
    "low_thresh_dummies = pd.get_dummies(df['Low Threshold Type'].astype(str), dtype=int)\n",
    "low_thresh_dummies.drop('nan', axis=1,inplace=True)\n",
    "low_thresh_dummies.rename(columns = {'SNAP, Other Nutrition Programs': 'SNAP_other'}, inplace=True)\n",
    "low_thresh_dummies.columns = 'Lo_thresh_'+low_thresh_dummies.columns\n",
    "\n",
    "df = pd.concat([df, low_thresh_dummies, hi_thresh_dummies],axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle the new dataframe\n",
    "Save the dataframe with all new features added (demographic percentages, interactions, logs, and dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pickled/feature_engineered_data.pickle', \"wb\") as output_file:\n",
    "    pickle.dump(df, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
